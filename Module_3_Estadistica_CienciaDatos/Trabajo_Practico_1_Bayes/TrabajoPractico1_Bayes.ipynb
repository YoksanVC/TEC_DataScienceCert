{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Practico 1: Bayes Ingenuo\n",
    "\n",
    "### Estudiantes:\n",
    "1. Sophia Contreras\n",
    "2. Yoksan Varela\n",
    "3. Mauro Viquez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias usadas en el codigo\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Para remover Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Para Pruebas unitarias\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones Generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_dataset(is_train = True):\n",
    "    \"\"\"Funcion para cargar CIFAR10 dataset, tomada del notebook BayesianModel_CIFAR_base.ipynb provisto en el curso.\n",
    "\n",
    "    Args:\n",
    "        is_train (bool, optional): Especifica si el modelo se esta entrenando. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Retorna los tensores con sus respectivos labels\n",
    "    \"\"\"\n",
    "    # Define a transformation to convert images to grayscale\n",
    "    transforms_1 = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Grayscale(num_output_channels=1)  # Convert to grayscale\n",
    "    ])\n",
    "    cifar_trainset = datasets.CIFAR10(root='./data', train = is_train, download = True, transform = transforms_1)\n",
    "  \n",
    "\n",
    "    # Initialize an empty list to store batches\n",
    "    all_data = []\n",
    "    train_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size = 64, shuffle=True)\n",
    "    # Iterate over the train_loader to fetch all batches\n",
    "    for batch in train_loader:\n",
    "        images, _ = batch  # Extract images from the batch\n",
    "        all_data.append(images)\n",
    "\n",
    "    # Concatenate all batches into a single tensor along the batch dimension\n",
    "    cifar_trainset_tensor = torch.round(torch.cat(all_data, dim=0) * 255)\n",
    "    cifar_labels = torch.tensor(cifar_trainset.targets)\n",
    "    print(\"cifar_trainset_tensor shape \", cifar_trainset_tensor.shape)\n",
    "    print(\"cifar_labels \", cifar_labels.shape)\n",
    "    return (cifar_trainset_tensor, cifar_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1, seccion A: Crear funcion calcular_probabilidad_priori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_probabilidad_priori(labels,categoria):\n",
    "    N = len(labels)\n",
    "    cuenta_categoria = np.count_nonzero(labels == categoria)\n",
    "    prob_marginal = cuenta_categoria/N\n",
    "    return torch.tensor(prob_marginal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1, seccion B: Pruebas unitarias para la funcion calcular_probabilidad_priori:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                         [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "# Creando un arreglo de 10 elementos conocidos con tres categorias: 1, 2 y 3\n",
    "test_tensor = torch.tensor([1,1,2,2,2,2,2,3,3,3])\n",
    "\n",
    "def test_calcular_probabilidad_priori_1() -> None:\n",
    "    # Primera prueba unitaria: Calcular la probabilidad de un 1 que es igual a 2/10 = 0.2\n",
    "    prob_1 = calcular_probabilidad_priori(test_tensor,1)\n",
    "    assert prob_1 == 0.2\n",
    "\n",
    "def test_calcular_probabilidad_priori_2() -> None:\n",
    "    # Segunda prueba unitaria: Calcular la probabilidad de un 2 que es igual a 5/10 = 0.5\n",
    "    prob_2 = calcular_probabilidad_priori(test_tensor,2)\n",
    "    assert prob_2 == 0.5\n",
    "\n",
    "def test_calcular_probabilidad_priori_3() -> None:\n",
    "    # Tercera prueba unitaria: Calcular la probabilidad de un 3 que es igual a 3/10 = 0.3\n",
    "    prob_3 = calcular_probabilidad_priori(test_tensor,3)\n",
    "    assert prob_3 == 0.3\n",
    "\n",
    "def test_calcular_probabilidad_priori_suma() -> None:\n",
    "    # Cuarta prueba unitaria: La suma de las todas las probabilidades de las categorias tiene que dar 1\n",
    "    prob_1 = calcular_probabilidad_priori(test_tensor,1)\n",
    "    prob_2 = calcular_probabilidad_priori(test_tensor,2)\n",
    "    prob_3 = calcular_probabilidad_priori(test_tensor,3)\n",
    "    assert (prob_1 + prob_2 + prob_3) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 1: Implementacion de la clasificacion multi-clase de imagenes con Byes Ingenuo usando histogramas\n",
    "\n",
    "Como primer paso, tenemos que cargar el set de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "cifar_trainset_tensor shape  torch.Size([50000, 1, 32, 32])\n",
      "cifar_labels  torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "# Cargando el dataset CIFAR10\n",
    "trainset_tensor,trainset_labels = load_cifar10_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con el dataset cargado, se analiza la distribucion de la informacion con respecto a las categorias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de los valores unicos de categorias: [0 1 2 3 4 5 6 7 8 9]\n",
      "Cuentas para cada uno de estos valores: [5000 5000 5000 5000 5000 5000 5000 5000 5000 5000]\n"
     ]
    }
   ],
   "source": [
    "valores_unicos, cuenta = np.unique(trainset_labels,return_counts=True)\n",
    "print(f\"Lista de los valores unicos de categorias: {valores_unicos}\")\n",
    "print(f\"Cuentas para cada uno de estos valores: {cuenta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay que hacer un sampling dado que los 50000 valores estan igualmente distribuidos en las 10 categorias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, creamos un arreglo del tamano de los datos con valores aleatorios, de forma que estos van a ser la forma sampleo ya que serian los indices a mantener\n",
    "arr_sampleo = np.random.choice(np.arange(len(trainset_labels)), 35000, replace=False)\n",
    "\n",
    "# Ahora se hace el sampleo tanto de imagenes como las categorias\n",
    "sample_labels = trainset_labels[arr_sampleo]\n",
    "sample_images = trainset_tensor[arr_sampleo,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of images shape  torch.Size([35000, 1, 32, 32])\n",
      "Sample of labels  torch.Size([35000])\n"
     ]
    }
   ],
   "source": [
    "# Ahora se comprueba que el sampling se haya hecho de forma de correcta\n",
    "print(\"Sample of images shape \", sample_images.shape)\n",
    "print(\"Sample of labels \", sample_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de los valores unicos de categorias: [0 1 2 3 4 5 6 7 8 9]\n",
      "Cuentas para cada uno de estos valores: [3509 3534 3477 3501 3463 3489 3576 3476 3484 3491]\n"
     ]
    }
   ],
   "source": [
    "# Verificamos que ya haya aleatoriedad en las categorias\n",
    "valores_unicos_sample, cuenta_sample = np.unique(sample_labels,return_counts=True)\n",
    "print(f\"Lista de los valores unicos de categorias: {valores_unicos_sample}\")\n",
    "print(f\"Cuentas para cada uno de estos valores: {cuenta_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con el muestreo completo, se procede a la siguiente seccion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1, seccion C: Enfoque basado en histogramas: Hypercubo de D x Z x K\n",
    "Donde D es la densidad de probablidad para cada pixel (1024 en total), Z es la intensidad de cada pixel (de 0 a 255) y K es la categoria a la que pertenece (total de 10 categorias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 4, 2, 1, 8, 1, 9, 9, 6, 3])\n",
      "tensor([[[[115., 120., 122.,  ..., 121., 120., 113.],\n",
      "          [115., 117., 117.,  ..., 113., 110., 107.],\n",
      "          [116., 117., 116.,  ..., 108., 106., 107.],\n",
      "          ...,\n",
      "          [120., 109., 101.,  ..., 100., 106., 102.],\n",
      "          [129., 129., 117.,  ..., 133., 127.,  97.],\n",
      "          [132., 130., 126.,  ..., 150., 127.,  93.]]],\n",
      "\n",
      "\n",
      "        [[[171., 221., 206.,  ..., 145., 236., 239.],\n",
      "          [ 84., 151., 132.,  ..., 177., 195., 188.],\n",
      "          [100.,  78., 167.,  ..., 170., 179., 207.],\n",
      "          ...,\n",
      "          [216., 203., 227.,  ..., 186., 196., 159.],\n",
      "          [113., 183., 225.,  ..., 139., 210., 229.],\n",
      "          [ 95., 134., 185.,  ..., 189., 233., 248.]]],\n",
      "\n",
      "\n",
      "        [[[137., 213.,  92.,  ...,  48.,  35.,  29.],\n",
      "          [ 83., 134.,  85.,  ...,  46.,  38.,  32.],\n",
      "          [ 76.,  90.,  67.,  ...,  46.,  40.,  35.],\n",
      "          ...,\n",
      "          [121., 121., 124.,  ..., 131., 130., 127.],\n",
      "          [121., 121., 123.,  ..., 127., 117., 107.],\n",
      "          [105., 115., 119.,  ..., 101., 103., 112.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[126., 120., 129.,  ..., 113., 105.,  99.],\n",
      "          [130., 127., 127.,  ..., 115., 112., 106.],\n",
      "          [133., 131., 130.,  ..., 117., 114., 110.],\n",
      "          ...,\n",
      "          [190., 188., 187.,  ..., 170., 164., 156.],\n",
      "          [183., 178., 175.,  ..., 167., 162., 156.],\n",
      "          [169., 162., 161.,  ..., 163., 158., 154.]]],\n",
      "\n",
      "\n",
      "        [[[ 81.,  78.,  52.,  ...,  94., 135., 187.],\n",
      "          [ 79.,  82.,  71.,  ..., 145., 156., 166.],\n",
      "          [ 77.,  79.,  93.,  ...,  99., 102., 103.],\n",
      "          ...,\n",
      "          [ 77.,  64.,  72.,  ...,  88.,  88.,  81.],\n",
      "          [ 79.,  62.,  64.,  ...,  91.,  90.,  88.],\n",
      "          [ 79.,  61.,  60.,  ...,  90.,  90.,  88.]]],\n",
      "\n",
      "\n",
      "        [[[254., 251., 252.,  ...,  78.,  50.,  60.],\n",
      "          [255., 252., 253.,  ..., 109.,  78.,  75.],\n",
      "          [249., 252., 254.,  ..., 125., 109.,  97.],\n",
      "          ...,\n",
      "          [ 70.,  40.,  40.,  ..., 175., 206., 151.],\n",
      "          [109., 101.,  96.,  ..., 153., 124., 122.],\n",
      "          [116., 112., 107.,  ..., 145., 118., 120.]]]])\n"
     ]
    }
   ],
   "source": [
    "# Pruebas para entender el conceptro del hypercube\n",
    "arr_subsampling = np.random.choice(10, 10, replace=False)\n",
    "#arr_subsampling = torch.tensor([3,1,1,1,4])\n",
    "\n",
    "# Miniset de pruebas\n",
    "subsample_labels = trainset_labels[arr_subsampling]\n",
    "subsample_images = trainset_tensor[arr_subsampling,:,:,:]\n",
    "\n",
    "print(subsample_labels)\n",
    "print(subsample_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_probability_per_category(data):\n",
    "    # Capturando la cantidad total de imagenes. La probabilidad de cada pixel la dicta este valor.\n",
    "    imagenes_qty = data.shape[0]\n",
    "\n",
    "    # Aplanando todos los tensores para convertirlos en 1D de 1024 pixeles, y convirtiendo los datos en int64 (la funcion bincount de numpy necesita ese formato)\n",
    "    tensor_flat = torch.flatten(data)\n",
    "    tensor_flat = torch.tensor(tensor_flat,dtype=torch.int64)\n",
    "\n",
    "    # Creando un tensor de # de imagenes x 1024 pixeles\n",
    "    reshaped_tensor = tensor_flat.reshape(imagenes_qty,1024)\n",
    "\n",
    "    # Haciendo la transpuesta para tener un tensor de 1024 x # de imagenes\n",
    "    transposed_tensor = np.transpose(reshaped_tensor)\n",
    "\n",
    "    # Contando la intensidad de luz por pixel para los 255 niveles. Esto genera un tensor de 1024 (cantidad de pixeles) x 255 (escala de gris)\n",
    "    count_tensor = np.apply_along_axis(np.bincount, axis=1, arr=transposed_tensor, minlength = 256)\n",
    "\n",
    "    # Finalmente, se divide todo el tensor entre el numero de imagenes procesadas para obtener la densidad de probabilidad m condicionada por t; y se retorna\n",
    "    count_tensor = torch.tensor(count_tensor,dtype=torch.float32)\n",
    "    probabilty_tensor = count_tensor/imagenes_qty\n",
    "\n",
    "    return torch.tensor(probabilty_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_density_hypercube(labels,data):\n",
    "    # Creando un hypercubo con las dimensiones pre-establecidas\n",
    "    D = 1024\n",
    "    Z = 256\n",
    "    K = 10\n",
    "    hypercube = torch.zeros([K,D,Z])\n",
    "\n",
    "    for category in range(K):\n",
    "        # Creando un tensor con los indices de todas las imagenes para la categoria actual\n",
    "        index_list = torch.tensor(labels == category)\n",
    "\n",
    "        # Creando un nuevo tensor con solo las imagenes que corresponden a la categoria actual\n",
    "        indexed_images = torch.tensor(data[index_list])\n",
    "        \n",
    "        # Enviar a procesar el nuevo tensor y popular el hypercubo solo si tiene imagenes, si no, continuar con la siguiente categoria\n",
    "        if(indexed_images.numel() != 0):\n",
    "            # Finalmente se popula el hypercubo\n",
    "            hypercube[category,:,:] = create_probability_per_category(indexed_images)\n",
    "    \n",
    "    return hypercube\n",
    "\n",
    "dataset_densities = create_density_hypercube(subsample_labels,subsample_images)\n",
    "\n",
    "with open(\"hipercubo.csv\", \"w+\") as file:\n",
    "    # Converting the array to a string and writing to the file\n",
    "    file.write(str(dataset_densities))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
